Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cores: 1 (use --cores to define parallelism)
Rules claiming more threads will be scaled down.
Job counts:
	count	jobs
	1	inference
	1

[Sun Aug  2 16:50:18 2020]
rule inference:
    input: ../data/roo/Head-SCC_signatures_ROO.RDS
    output: ../data/inference/Head-SCC_signatures_2000_LNMROO.RData
    log: logs/inference/Head-SCC_signatures_2000_LNM.log
    jobid: 0
    wildcards: cancer_type=Head-SCC, feature_type=signatures, nits=2000, model=LNM

module load miniconda3-4.5.4-gcc-5.4.0-hivczbz
#source activate rstan_env
~/.conda/envs/rstan_env/bin/Rscript --vanilla 2_inference/fit_PCAWG.R --cancertype Head-SCC --typedata signatures --infile '../data/roo/Head-SCC_signatures_ROO.RDS' --output '../data/inference/Head-SCC_signatures_2000_LNMROO.RData' --niterations 2000 --model LNM
#conda deactivate
Loading required package: ggplot2
Registered S3 methods overwritten by 'ggplot2':
  method         from 
  [.quosures     rlang
  c.quosures     rlang
  print.quosures rlang
Loading required package: StanHeaders
rstan (Version 2.18.2, GitRev: 2e1f913d3ca3)
For execution on a local, multicore CPU with excess RAM we recommend calling
options(mc.cores = parallel::detectCores()).
To avoid recompilation of unchanged Stan programs, we recommend calling
rstan_options(auto_write = TRUE)
$status
[1] TRUE

$model_cppname
[1] "model1fb34e323adb_stan_LNM_ME"

$cppcode
[1] "// Code generated by Stan version 2.18.1\n\n#include <stan/model/model_header.hpp>\n\nnamespace model1fb34e323adb_stan_LNM_ME_namespace {\n\nusing std::istream;\nusing std::string;\nusing std::stringstream;\nusing std::vector;\nusing stan::io::dump;\nusing stan::math::lgamma;\nusing stan::model::prob_grad;\nusing namespace stan::math;\n\nstatic int current_statement_begin__;\n\nstan::io::program_reader prog_reader__() {\n    stan::io::program_reader reader;\n    reader.add_event(0, 0, \"start\", \"model1fb34e323adb_stan_LNM_ME\");\n    reader.add_event(60, 58, \"end\", \"model1fb34e323adb_stan_LNM_ME\");\n    return reader;\n}\n\nclass model1fb34e323adb_stan_LNM_ME : public prob_grad {\nprivate:\n    int d;\n    int n;\n    vector<vector<int> > w;\n    matrix_d x;\n    matrix_d Z;\npublic:\n    model1fb34e323adb_stan_LNM_ME(stan::io::var_context& context__,\n        std::ostream* pstream__ = 0)\n        : prob_grad(0) {\n        ctor_body(context__, 0, pstream__);\n    }\n\n    model1fb34e323adb_stan_LNM_ME(stan::io::var_context& context__,\n        unsigned int random_seed__,\n        std::ostream* pstream__ = 0)\n        : prob_grad(0) {\n        ctor_body(context__, random_seed__, pstream__);\n    }\n\n    void ctor_body(stan::io::var_context& context__,\n                   unsigned int random_seed__,\n                   std::ostream* pstream__) {\n        typedef double local_scalar_t__;\n\n        boost::ecuyer1988 base_rng__ =\n          stan::services::util::create_rng(random_seed__, 0);\n        (void) base_rng__;  // suppress unused var warning\n\n        current_statement_begin__ = -1;\n\n        static const char* function__ = \"model1fb34e323adb_stan_LNM_ME_namespace::model1fb34e323adb_stan_LNM_ME\";\n        (void) function__;  // dummy to suppress unused var warning\n        size_t pos__;\n        (void) pos__;  // dummy to suppress unused var warning\n        std::vector<int> vals_i__;\n        std::vector<double> vals_r__;\n        local_scalar_t__ DUMMY_VAR__(std::numeric_limits<double>::quiet_NaN());\n        (void) DUMMY_VAR__;  // suppress unused var warning\n\n        // initialize member variables\n        try {\n            current_statement_begin__ = 3;\n            context__.validate_dims(\"data initialization\", \"d\", \"int\", context__.to_vec());\n            d = int(0);\n            vals_i__ = context__.vals_i(\"d\");\n            pos__ = 0;\n            d = vals_i__[pos__++];\n            current_statement_begin__ = 4;\n            context__.validate_dims(\"data initialization\", \"n\", \"int\", context__.to_vec());\n            n = int(0);\n            vals_i__ = context__.vals_i(\"n\");\n            pos__ = 0;\n            n = vals_i__[pos__++];\n            current_statement_begin__ = 5;\n            validate_non_negative_index(\"w\", \"(2 * n)\", (2 * n));\n            validate_non_negative_index(\"w\", \"d\", d);\n            context__.validate_dims(\"data initialization\", \"w\", \"int\", context__.to_vec((2 * n),d));\n            validate_non_negative_index(\"w\", \"(2 * n)\", (2 * n));\n            validate_non_negative_index(\"w\", \"d\", d);\n            w = std::vector<std::vector<int> >((2 * n),std::vector<int>(d,int(0)));\n            vals_i__ = context__.vals_i(\"w\");\n            pos__ = 0;\n            size_t w_limit_1__ = d;\n            for (size_t i_1__ = 0; i_1__ < w_limit_1__; ++i_1__) {\n                size_t w_limit_0__ = (2 * n);\n                for (size_t i_0__ = 0; i_0__ < w_limit_0__; ++i_0__) {\n                    w[i_0__][i_1__] = vals_i__[pos__++];\n                }\n            }\n            current_statement_begin__ = 6;\n            validate_non_negative_index(\"x\", \"2\", 2);\n            validate_non_negative_index(\"x\", \"(2 * n)\", (2 * n));\n            context__.validate_dims(\"data initialization\", \"x\", \"matrix_d\", context__.to_vec(2,(2 * n)));\n            validate_non_negative_index(\"x\", \"2\", 2);\n            validate_non_negative_index(\"x\", \"(2 * n)\", (2 * n));\n            x = matrix_d(static_cast<Eigen::VectorXd::Index>(2),static_cast<Eigen::VectorXd::Index>((2 * n)));\n            vals_r__ = context__.vals_r(\"x\");\n            pos__ = 0;\n            size_t x_m_mat_lim__ = 2;\n            size_t x_n_mat_lim__ = (2 * n);\n            for (size_t n_mat__ = 0; n_mat__ < x_n_mat_lim__; ++n_mat__) {\n                for (size_t m_mat__ = 0; m_mat__ < x_m_mat_lim__; ++m_mat__) {\n                    x(m_mat__,n_mat__) = vals_r__[pos__++];\n                }\n            }\n            current_statement_begin__ = 7;\n            validate_non_negative_index(\"Z\", \"n\", n);\n            validate_non_negative_index(\"Z\", \"(2 * n)\", (2 * n));\n            context__.validate_dims(\"data initialization\", \"Z\", \"matrix_d\", context__.to_vec(n,(2 * n)));\n            validate_non_negative_index(\"Z\", \"n\", n);\n            validate_non_negative_index(\"Z\", \"(2 * n)\", (2 * n));\n            Z = matrix_d(static_cast<Eigen::VectorXd::Index>(n),static_cast<Eigen::VectorXd::Index>((2 * n)));\n            vals_r__ = context__.vals_r(\"Z\");\n            pos__ = 0;\n            size_t Z_m_mat_lim__ = n;\n            size_t Z_n_mat_lim__ = (2 * n);\n            for (size_t n_mat__ = 0; n_mat__ < Z_n_mat_lim__; ++n_mat__) {\n                for (size_t m_mat__ = 0; m_mat__ < Z_m_mat_lim__; ++m_mat__) {\n                    Z(m_mat__,n_mat__) = vals_r__[pos__++];\n                }\n            }\n\n            // validate, data variables\n            current_statement_begin__ = 3;\n            check_greater_or_equal(function__,\"d\",d,1);\n            current_statement_begin__ = 4;\n            check_greater_or_equal(function__,\"n\",n,1);\n            current_statement_begin__ = 5;\n            current_statement_begin__ = 6;\n            current_statement_begin__ = 7;\n            // initialize data variables\n\n\n            // validate transformed data\n\n            // validate, set parameter ranges\n            num_params_r__ = 0U;\n            param_ranges_i__.clear();\n            current_statement_begin__ = 12;\n            validate_non_negative_index(\"Sigma\", \"(d - 1)\", (d - 1));\n            num_params_r__ += (((d - 1) * ((d - 1) - 1)) / 2 + (d - 1));\n            current_statement_begin__ = 13;\n            validate_non_negative_index(\"v\", \"(d - 1)\", (d - 1));\n            validate_non_negative_index(\"v\", \"(2 * n)\", (2 * n));\n            num_params_r__ += (d - 1) * (2 * n);\n            current_statement_begin__ = 14;\n            ++num_params_r__;\n            current_statement_begin__ = 15;\n        validate_non_negative_index(\"beta\", \"2\", 2);\n            validate_non_negative_index(\"beta\", \"(d - 1)\", (d - 1));\n            num_params_r__ += 2 * (d - 1);\n            current_statement_begin__ = 16;\n            validate_non_negative_index(\"u\", \"n\", n);\n            num_params_r__ += n;\n        } catch (const std::exception& e) {\n            stan::lang::rethrow_located(e, current_statement_begin__, prog_reader__());\n            // Next line prevents compiler griping about no return\n            throw std::runtime_error(\"*** IF YOU SEE THIS, PLEASE REPORT A BUG ***\");\n        }\n    }\n\n    ~model1fb34e323adb_stan_LNM_ME() { }\n\n\n    void transform_inits(const stan::io::var_context& context__,\n                         std::vector<int>& params_i__,\n                         std::vector<double>& params_r__,\n                         std::ostream* pstream__) const {\n        stan::io::writer<double> writer__(params_r__,params_i__);\n        size_t pos__;\n        (void) pos__; // dummy call to supress warning\n        std::vector<double> vals_r__;\n        std::vector<int> vals_i__;\n\n        if (!(context__.contains_r(\"Sigma\")))\n            throw std::runtime_error(\"variable Sigma missing\");\n        vals_r__ = context__.vals_r(\"Sigma\");\n        pos__ = 0U;\n        validate_non_negative_index(\"Sigma\", \"(d - 1)\", (d - 1));\n        validate_non_negative_index(\"Sigma\", \"(d - 1)\", (d - 1));\n        context__.validate_dims(\"initialization\", \"Sigma\", \"matrix_d\", context__.to_vec((d - 1),(d - 1)));\n        matrix_d Sigma(static_cast<Eigen::VectorXd::Index>((d - 1)),static_cast<Eigen::VectorXd::Index>((d - 1)));\n        for (int j2__ = 0U; j2__ < (d - 1); ++j2__)\n            for (int j1__ = 0U; j1__ < (d - 1); ++j1__)\n                Sigma(j1__,j2__) = vals_r__[pos__++];\n        try {\n            writer__.cov_matrix_unconstrain(Sigma);\n        } catch (const std::exception& e) { \n            throw std::runtime_error(std::string(\"Error transforming variable Sigma: \") + e.what());\n        }\n\n        if (!(context__.contains_r(\"v\")))\n            throw std::runtime_error(\"variable v missing\");\n        vals_r__ = context__.vals_r(\"v\");\n        pos__ = 0U;\n        validate_non_negative_index(\"v\", \"(2 * n)\", (2 * n));\n        validate_non_negative_index(\"v\", \"(d - 1)\", (d - 1));\n        context__.validate_dims(\"initialization\", \"v\", \"vector_d\", context__.to_vec((2 * n),(d - 1)));\n        std::vector<vector_d> v((2 * n),vector_d(static_cast<Eigen::VectorXd::Index>((d - 1))));\n        for (int j1__ = 0U; j1__ < (d - 1); ++j1__)\n            for (int i0__ = 0U; i0__ < (2 * n); ++i0__)\n                v[i0__](j1__) = vals_r__[pos__++];\n        for (int i0__ = 0U; i0__ < (2 * n); ++i0__)\n            try {\n            writer__.vector_unconstrain(v[i0__]);\n        } catch (const std::exception& e) { \n            throw std::runtime_error(std::string(\"Error transforming variable v: \") + e.what());\n        }\n\n        if (!(context__.contains_r(\"var_u\")))\n            throw std::runtime_error(\"variable var_u missing\");\n        vals_r__ = context__.vals_r(\"var_u\");\n        pos__ = 0U;\n        context__.validate_dims(\"initialization\", \"var_u\", \"double\", context__.to_vec());\n        double var_u(0);\n        var_u = vals_r__[pos__++];\n        try {\n            writer__.scalar_lb_unconstrain(0,var_u);\n        } catch (const std::exception& e) { \n            throw std::runtime_error(std::string(\"Error transforming variable var_u: \") + e.what());\n        }\n\n        if (!(context__.contains_r(\"beta\")))\n            throw std::runtime_error(\"variable beta missing\");\n        vals_r__ = context__.vals_r(\"beta\");\n        pos__ = 0U;\n        validate_non_negative_index(\"beta\", \"2\", 2);\n        validate_non_negative_index(\"beta\", \"(d - 1)\", (d - 1));\n        context__.validate_dims(\"initialization\", \"beta\", \"matrix_d\", context__.to_vec(2,(d - 1)));\n        matrix_d beta(static_cast<Eigen::VectorXd::Index>(2),static_cast<Eigen::VectorXd::Index>((d - 1)));\n        for (int j2__ = 0U; j2__ < (d - 1); ++j2__)\n            for (int j1__ = 0U; j1__ < 2; ++j1__)\n                beta(j1__,j2__) = vals_r__[pos__++];\n        try {\n            writer__.matrix_unconstrain(beta);\n        } catch (const std::exception& e) { \n            throw std::runtime_error(std::string(\"Error transforming variable beta: \") + e.what());\n        }\n\n        if (!(context__.contains_r(\"u\")))\n            throw std::runtime_error(\"variable u missing\");\n        vals_r__ = context__.vals_r(\"u\");\n        pos__ = 0U;\n        validate_non_negative_index(\"u\", \"n\", n);\n        context__.validate_dims(\"initialization\", \"u\", \"vector_d\", context__.to_vec(n));\n        vector_d u(static_cast<Eigen::VectorXd::Index>(n));\n        for (int j1__ = 0U; j1__ < n; ++j1__)\n            u(j1__) = vals_r__[pos__++];\n        try {\n            writer__.vector_unconstrain(u);\n        } catch (const std::exception& e) { \n            throw std::runtime_error(std::string(\"Error transforming variable u: \") + e.what());\n        }\n\n        params_r__ = writer__.data_r();\n        params_i__ = writer__.data_i();\n    }\n\n    void transform_inits(const stan::io::var_context& context,\n                         Eigen::Matrix<double,Eigen::Dynamic,1>& params_r,\n                         std::ostream* pstream__) const {\n      std::vector<double> params_r_vec;\n      std::vector<int> params_i_vec;\n      transform_inits(context, params_i_vec, params_r_vec, pstream__);\n      params_r.resize(params_r_vec.size());\n      for (int i = 0; i < params_r.size(); ++i)\n        params_r(i) = params_r_vec[i];\n    }\n\n\n    template <bool propto__, bool jacobian__, typename T__>\n    T__ log_prob(vector<T__>& params_r__,\n                 vector<int>& params_i__,\n                 std::ostream* pstream__ = 0) const {\n\n        typedef T__ local_scalar_t__;\n\n        local_scalar_t__ DUMMY_VAR__(std::numeric_limits<double>::quiet_NaN());\n        (void) DUMMY_VAR__;  // suppress unused var warning\n\n        T__ lp__(0.0);\n        stan::math::accumulator<T__> lp_accum__;\n\n        try {\n            // model parameters\n            stan::io::reader<local_scalar_t__> in__(params_r__,params_i__);\n\n            Eigen::Matrix<local_scalar_t__,Eigen::Dynamic,Eigen::Dynamic>  Sigma;\n            (void) Sigma;  // dummy to suppress unused var warning\n            if (jacobian__)\n                Sigma = in__.cov_matrix_constrain((d - 1),lp__);\n            else\n                Sigma = in__.cov_matrix_constrain((d - 1));\n\n            vector<Eigen::Matrix<local_scalar_t__,Eigen::Dynamic,1> > v;\n            size_t dim_v_0__ = (2 * n);\n            v.reserve(dim_v_0__);\n            for (size_t k_0__ = 0; k_0__ < dim_v_0__; ++k_0__) {\n                if (jacobian__)\n                    v.push_back(in__.vector_constrain((d - 1),lp__));\n                else\n                    v.push_back(in__.vector_constrain((d - 1)));\n            }\n\n            local_scalar_t__ var_u;\n            (void) var_u;  // dummy to suppress unused var warning\n            if (jacobian__)\n                var_u = in__.scalar_lb_constrain(0,lp__);\n            else\n                var_u = in__.scalar_lb_constrain(0);\n\n            Eigen::Matrix<local_scalar_t__,Eigen::Dynamic,Eigen::Dynamic>  beta;\n            (void) beta;  // dummy to suppress unused var warning\n            if (jacobian__)\n                beta = in__.matrix_constrain(2,(d - 1),lp__);\n            else\n                beta = in__.matrix_constrain(2,(d - 1));\n\n            Eigen::Matrix<local_scalar_t__,Eigen::Dynamic,1>  u;\n            (void) u;  // dummy to suppress unused var warning\n            if (jacobian__)\n                u = in__.vector_constrain(n,lp__);\n            else\n                u = in__.vector_constrain(n);\n\n\n            // transformed parameters\n            current_statement_begin__ = 20;\n            validate_non_negative_index(\"mu\", \"(2 * n)\", (2 * n));\n            validate_non_negative_index(\"mu\", \"(d - 1)\", (d - 1));\n            Eigen::Matrix<local_scalar_t__,Eigen::Dynamic,Eigen::Dynamic>  mu(static_cast<Eigen::VectorXd::Index>((2 * n)),static_cast<Eigen::VectorXd::Index>((d - 1)));\n            (void) mu;  // dummy to suppress unused var warning\n\n            stan::math::initialize(mu, DUMMY_VAR__);\n            stan::math::fill(mu,DUMMY_VAR__);\n            current_statement_begin__ = 21;\n            validate_non_negative_index(\"theta\", \"d\", d);\n            validate_non_negative_index(\"theta\", \"(2 * n)\", (2 * n));\n            vector<Eigen::Matrix<local_scalar_t__,Eigen::Dynamic,1> > theta((2 * n), (Eigen::Matrix<local_scalar_t__,Eigen::Dynamic,1> (static_cast<Eigen::VectorXd::Index>(d))));\n            stan::math::initialize(theta, DUMMY_VAR__);\n            stan::math::fill(theta,DUMMY_VAR__);\n            current_statement_begin__ = 22;\n            validate_non_negative_index(\"full_v\", \"d\", d);\n            validate_non_negative_index(\"full_v\", \"(2 * n)\", (2 * n));\n            vector<Eigen::Matrix<local_scalar_t__,Eigen::Dynamic,1> > full_v((2 * n), (Eigen::Matrix<local_scalar_t__,Eigen::Dynamic,1> (static_cast<Eigen::VectorXd::Index>(d))));\n            stan::math::initialize(full_v, DUMMY_VAR__);\n            stan::math::fill(full_v,DUMMY_VAR__);\n\n\n            current_statement_begin__ = 24;\n            for (int i = 1; i <= (2 * n); ++i) {\n\n                current_statement_begin__ = 25;\n                for (int j = 1; j <= (d - 1); ++j) {\n\n                    current_statement_begin__ = 26;\n                    stan::model::assign(full_v, \n                                stan::model::cons_list(stan::model::index_uni(i), stan::model::cons_list(stan::model::index_uni(j), stan::model::nil_index_list())), \n                                get_base1(get_base1(v,i,\"v\",1),j,\"v\",2), \n                                \"assigning variable full_v\");\n                }\n                current_statement_begin__ = 28;\n                stan::model::assign(full_v, \n                            stan::model::cons_list(stan::model::index_uni(i), stan::model::cons_list(stan::model::index_uni(d), stan::model::nil_index_list())), \n                            0, \n                            \"assigning variable full_v\");\n            }\n            current_statement_begin__ = 31;\n            for (int l = 1; l <= (2 * n); ++l) {\n\n                current_statement_begin__ = 32;\n                stan::model::assign(theta, \n                            stan::model::cons_list(stan::model::index_uni(l), stan::model::cons_list(stan::model::index_omni(), stan::model::nil_index_list())), \n                            softmax(stan::model::rvalue(full_v, stan::model::cons_list(stan::model::index_uni(l), stan::model::cons_list(stan::model::index_omni(), stan::model::nil_index_list())), \"full_v\")), \n                            \"assigning variable theta\");\n            }\n            current_statement_begin__ = 35;\n            stan::math::assign(mu, add(multiply(transpose(x),beta),multiply(transpose(Z),rep_matrix(u,(d - 1)))));\n\n            // validate transformed parameters\n            for (int i0__ = 0; i0__ < (2 * n); ++i0__) {\n                for (int i1__ = 0; i1__ < (d - 1); ++i1__) {\n                    if (stan::math::is_uninitialized(mu(i0__,i1__))) {\n                        std::stringstream msg__;\n                        msg__ << \"Undefined transformed parameter: mu\" << '[' << i0__ << ']' << '[' << i1__ << ']';\n                        throw std::runtime_error(msg__.str());\n                    }\n                }\n            }\n            for (int i0__ = 0; i0__ < (2 * n); ++i0__) {\n                for (int i1__ = 0; i1__ < d; ++i1__) {\n                    if (stan::math::is_uninitialized(theta[i0__](i1__))) {\n                        std::stringstream msg__;\n                        msg__ << \"Undefined transformed parameter: theta\" << '[' << i0__ << ']' << '[' << i1__ << ']';\n                        throw std::runtime_error(msg__.str());\n                    }\n                }\n            }\n            for (int i0__ = 0; i0__ < (2 * n); ++i0__) {\n                for (int i1__ = 0; i1__ < d; ++i1__) {\n                    if (stan::math::is_uninitialized(full_v[i0__](i1__))) {\n                        std::stringstream msg__;\n                        msg__ << \"Undefined transformed parameter: full_v\" << '[' << i0__ << ']' << '[' << i1__ << ']';\n                        throw std::runtime_error(msg__.str());\n                    }\n                }\n            }\n\n            const char* function__ = \"validate transformed params\";\n            (void) function__;  // dummy to suppress unused var warning\n            current_statement_begin__ = 20;\n            current_statement_begin__ = 21;\n            for (int k0__ = 0; k0__ < (2 * n); ++k0__) {\n                stan::math::check_simplex(function__,\"theta[k0__]\",theta[k0__]);\n            }\n            current_statement_begin__ = 22;\n\n            // model body\n\n            current_statement_begin__ = 41;\n            lp_accum__.add(gamma_log<propto__>(var_u, 5, 5));\n            current_statement_begin__ = 44;\n            lp_accum__.add(normal_log<propto__>(u, 0, stan::math::sqrt(var_u)));\n            current_statement_begin__ = 46;\n            for (int d_it = 1; d_it <= (d - 1); ++d_it) {\n\n                current_statement_begin__ = 47;\n                lp_accum__.add(uniform_log<propto__>(stan::model::rvalue(beta, stan::model::cons_list(stan::model::index_omni(), stan::model::cons_list(stan::model::index_uni(d_it), stan::model::nil_index_list())), \"beta\"), -(5), 5));\n            }\n            current_statement_begin__ = 50;\n            for (int i = 1; i <= (2 * n); ++i) {\n\n                current_statement_begin__ = 51;\n                lp_accum__.add(multi_normal_log<propto__>(stan::model::rvalue(v, stan::model::cons_list(stan::model::index_uni(i), stan::model::cons_list(stan::model::index_omni(), stan::model::nil_index_list())), \"v\"), stan::model::rvalue(mu, stan::model::cons_list(stan::model::index_uni(i), stan::model::cons_list(stan::model::index_omni(), stan::model::nil_index_list())), \"mu\"), Sigma));\n            }\n            current_statement_begin__ = 54;\n            for (int i = 1; i <= (2 * n); ++i) {\n\n                current_statement_begin__ = 55;\n                lp_accum__.add(multinomial_log<propto__>(stan::model::rvalue(w, stan::model::cons_list(stan::model::index_uni(i), stan::model::cons_list(stan::model::index_omni(), stan::model::nil_index_list())), \"w\"), stan::model::rvalue(theta, stan::model::cons_list(stan::model::index_uni(i), stan::model::cons_list(stan::model::index_omni(), stan::model::nil_index_list())), \"theta\")));\n            }\n\n        } catch (const std::exception& e) {\n            stan::lang::rethrow_located(e, current_statement_begin__, prog_reader__());\n            // Next line prevents compiler griping about no return\n            throw std::runtime_error(\"*** IF YOU SEE THIS, PLEASE REPORT A BUG ***\");\n        }\n\n        lp_accum__.add(lp__);\n        return lp_accum__.sum();\n\n    } // log_prob()\n\n    template <bool propto, bool jacobian, typename T_>\n    T_ log_prob(Eigen::Matrix<T_,Eigen::Dynamic,1>& params_r,\n               std::ostream* pstream = 0) const {\n      std::vector<T_> vec_params_r;\n      vec_params_r.reserve(params_r.size());\n      for (int i = 0; i < params_r.size(); ++i)\n        vec_params_r.push_back(params_r(i));\n      std::vector<int> vec_params_i;\n      return log_prob<propto,jacobian,T_>(vec_params_r, vec_params_i, pstream);\n    }\n\n\n    void get_param_names(std::vector<std::string>& names__) const {\n        names__.resize(0);\n        names__.push_back(\"Sigma\");\n        names__.push_back(\"v\");\n        names__.push_back(\"var_u\");\n        names__.push_back(\"beta\");\n        names__.push_back(\"u\");\n        names__.push_back(\"mu\");\n        names__.push_back(\"theta\");\n        names__.push_back(\"full_v\");\n    }\n\n\n    void get_dims(std::vector<std::vector<size_t> >& dimss__) const {\n        dimss__.resize(0);\n        std::vector<size_t> dims__;\n        dims__.resize(0);\n        dims__.push_back((d - 1));\n        dims__.push_back((d - 1));\n        dimss__.push_back(dims__);\n        dims__.resize(0);\n        dims__.push_back((2 * n));\n        dims__.push_back((d - 1));\n        dimss__.push_back(dims__);\n        dims__.resize(0);\n        dimss__.push_back(dims__);\n        dims__.resize(0);\n        dims__.push_back(2);\n        dims__.push_back((d - 1));\n        dimss__.push_back(dims__);\n        dims__.resize(0);\n        dims__.push_back(n);\n        dimss__.push_back(dims__);\n        dims__.resize(0);\n        dims__.push_back((2 * n));\n        dims__.push_back((d - 1));\n        dimss__.push_back(dims__);\n        dims__.resize(0);\n        dims__.push_back((2 * n));\n        dims__.push_back(d);\n        dimss__.push_back(dims__);\n        dims__.resize(0);\n        dims__.push_back((2 * n));\n        dims__.push_back(d);\n        dimss__.push_back(dims__);\n    }\n\n    template <typename RNG>\n    void write_array(RNG& base_rng__,\n                     std::vector<double>& params_r__,\n                     std::vector<int>& params_i__,\n                     std::vector<double>& vars__,\n                     bool include_tparams__ = true,\n                     bool include_gqs__ = true,\n                     std::ostream* pstream__ = 0) const {\n        typedef double local_scalar_t__;\n\n        vars__.resize(0);\n        stan::io::reader<local_scalar_t__> in__(params_r__,params_i__);\n        static const char* function__ = \"model1fb34e323adb_stan_LNM_ME_namespace::write_array\";\n        (void) function__;  // dummy to suppress unused var warning\n        // read-transform, write parameters\n        matrix_d Sigma = in__.cov_matrix_constrain((d - 1));\n        vector<vector_d> v;\n        size_t dim_v_0__ = (2 * n);\n        for (size_t k_0__ = 0; k_0__ < dim_v_0__; ++k_0__) {\n            v.push_back(in__.vector_constrain((d - 1)));\n        }\n        double var_u = in__.scalar_lb_constrain(0);\n        matrix_d beta = in__.matrix_constrain(2,(d - 1));\n        vector_d u = in__.vector_constrain(n);\n            for (int k_1__ = 0; k_1__ < (d - 1); ++k_1__) {\n                for (int k_0__ = 0; k_0__ < (d - 1); ++k_0__) {\n                vars__.push_back(Sigma(k_0__, k_1__));\n                }\n            }\n            for (int k_1__ = 0; k_1__ < (d - 1); ++k_1__) {\n                for (int k_0__ = 0; k_0__ < (2 * n); ++k_0__) {\n                vars__.push_back(v[k_0__][k_1__]);\n                }\n            }\n        vars__.push_back(var_u);\n            for (int k_1__ = 0; k_1__ < (d - 1); ++k_1__) {\n                for (int k_0__ = 0; k_0__ < 2; ++k_0__) {\n                vars__.push_back(beta(k_0__, k_1__));\n                }\n            }\n            for (int k_0__ = 0; k_0__ < n; ++k_0__) {\n            vars__.push_back(u[k_0__]);\n            }\n\n        // declare and define transformed parameters\n        double lp__ = 0.0;\n        (void) lp__;  // dummy to suppress unused var warning\n        stan::math::accumulator<double> lp_accum__;\n\n        local_scalar_t__ DUMMY_VAR__(std::numeric_limits<double>::quiet_NaN());\n        (void) DUMMY_VAR__;  // suppress unused var warning\n\n        try {\n            current_statement_begin__ = 20;\n            validate_non_negative_index(\"mu\", \"(2 * n)\", (2 * n));\n            validate_non_negative_index(\"mu\", \"(d - 1)\", (d - 1));\n            Eigen::Matrix<local_scalar_t__,Eigen::Dynamic,Eigen::Dynamic>  mu(static_cast<Eigen::VectorXd::Index>((2 * n)),static_cast<Eigen::VectorXd::Index>((d - 1)));\n            (void) mu;  // dummy to suppress unused var warning\n\n            stan::math::initialize(mu, DUMMY_VAR__);\n            stan::math::fill(mu,DUMMY_VAR__);\n            current_statement_begin__ = 21;\n            validate_non_negative_index(\"theta\", \"d\", d);\n            validate_non_negative_index(\"theta\", \"(2 * n)\", (2 * n));\n            vector<Eigen::Matrix<local_scalar_t__,Eigen::Dynamic,1> > theta((2 * n), (Eigen::Matrix<local_scalar_t__,Eigen::Dynamic,1> (static_cast<Eigen::VectorXd::Index>(d))));\n            stan::math::initialize(theta, DUMMY_VAR__);\n            stan::math::fill(theta,DUMMY_VAR__);\n            current_statement_begin__ = 22;\n            validate_non_negative_index(\"full_v\", \"d\", d);\n            validate_non_negative_index(\"full_v\", \"(2 * n)\", (2 * n));\n            vector<Eigen::Matrix<local_scalar_t__,Eigen::Dynamic,1> > full_v((2 * n), (Eigen::Matrix<local_scalar_t__,Eigen::Dynamic,1> (static_cast<Eigen::VectorXd::Index>(d))));\n            stan::math::initialize(full_v, DUMMY_VAR__);\n            stan::math::fill(full_v,DUMMY_VAR__);\n\n\n            current_statement_begin__ = 24;\n            for (int i = 1; i <= (2 * n); ++i) {\n\n                current_statement_begin__ = 25;\n                for (int j = 1; j <= (d - 1); ++j) {\n\n                    current_statement_begin__ = 26;\n                    stan::model::assign(full_v, \n                                stan::model::cons_list(stan::model::index_uni(i), stan::model::cons_list(stan::model::index_uni(j), stan::model::nil_index_list())), \n                                get_base1(get_base1(v,i,\"v\",1),j,\"v\",2), \n                                \"assigning variable full_v\");\n                }\n                current_statement_begin__ = 28;\n                stan::model::assign(full_v, \n                            stan::model::cons_list(stan::model::index_uni(i), stan::model::cons_list(stan::model::index_uni(d), stan::model::nil_index_list())), \n                            0, \n                            \"assigning variable full_v\");\n            }\n            current_statement_begin__ = 31;\n            for (int l = 1; l <= (2 * n); ++l) {\n\n                current_statement_begin__ = 32;\n                stan::model::assign(theta, \n                            stan::model::cons_list(stan::model::index_uni(l), stan::model::cons_list(stan::model::index_omni(), stan::model::nil_index_list())), \n                            softmax(stan::model::rvalue(full_v, stan::model::cons_list(stan::model::index_uni(l), stan::model::cons_list(stan::model::index_omni(), stan::model::nil_index_list())), \"full_v\")), \n                            \"assigning variable theta\");\n            }\n            current_statement_begin__ = 35;\n            stan::math::assign(mu, add(multiply(transpose(x),beta),multiply(transpose(Z),rep_matrix(u,(d - 1)))));\n\n            // validate transformed parameters\n            current_statement_begin__ = 20;\n            current_statement_begin__ = 21;\n            for (int k0__ = 0; k0__ < (2 * n); ++k0__) {\n                stan::math::check_simplex(function__,\"theta[k0__]\",theta[k0__]);\n            }\n            current_statement_begin__ = 22;\n\n            // write transformed parameters\n            if (include_tparams__) {\n            for (int k_1__ = 0; k_1__ < (d - 1); ++k_1__) {\n                for (int k_0__ = 0; k_0__ < (2 * n); ++k_0__) {\n                vars__.push_back(mu(k_0__, k_1__));\n                }\n            }\n            for (int k_1__ = 0; k_1__ < d; ++k_1__) {\n                for (int k_0__ = 0; k_0__ < (2 * n); ++k_0__) {\n                vars__.push_back(theta[k_0__][k_1__]);\n                }\n            }\n            for (int k_1__ = 0; k_1__ < d; ++k_1__) {\n                for (int k_0__ = 0; k_0__ < (2 * n); ++k_0__) {\n                vars__.push_back(full_v[k_0__][k_1__]);\n                }\n            }\n            }\n            if (!include_gqs__) return;\n            // declare and define generated quantities\n\n\n\n            // validate generated quantities\n\n            // write generated quantities\n        } catch (const std::exception& e) {\n            stan::lang::rethrow_located(e, current_statement_begin__, prog_reader__());\n            // Next line prevents compiler griping about no return\n            throw std::runtime_error(\"*** IF YOU SEE THIS, PLEASE REPORT A BUG ***\");\n        }\n    }\n\n    template <typename RNG>\n    void write_array(RNG& base_rng,\n                     Eigen::Matrix<double,Eigen::Dynamic,1>& params_r,\n                     Eigen::Matrix<double,Eigen::Dynamic,1>& vars,\n                     bool include_tparams = true,\n                     bool include_gqs = true,\n                     std::ostream* pstream = 0) const {\n      std::vector<double> params_r_vec(params_r.size());\n      for (int i = 0; i < params_r.size(); ++i)\n        params_r_vec[i] = params_r(i);\n      std::vector<double> vars_vec;\n      std::vector<int> params_i_vec;\n      write_array(base_rng,params_r_vec,params_i_vec,vars_vec,include_tparams,include_gqs,pstream);\n      vars.resize(vars_vec.size());\n      for (int i = 0; i < vars.size(); ++i)\n        vars(i) = vars_vec[i];\n    }\n\n    static std::string model_name() {\n        return \"model1fb34e323adb_stan_LNM_ME\";\n    }\n\n\n    void constrained_param_names(std::vector<std::string>& param_names__,\n                                 bool include_tparams__ = true,\n                                 bool include_gqs__ = true) const {\n        std::stringstream param_name_stream__;\n        for (int k_1__ = 1; k_1__ <= (d - 1); ++k_1__) {\n            for (int k_0__ = 1; k_0__ <= (d - 1); ++k_0__) {\n                param_name_stream__.str(std::string());\n                param_name_stream__ << \"Sigma\" << '.' << k_0__ << '.' << k_1__;\n                param_names__.push_back(param_name_stream__.str());\n            }\n        }\n        for (int k_1__ = 1; k_1__ <= (d - 1); ++k_1__) {\n            for (int k_0__ = 1; k_0__ <= (2 * n); ++k_0__) {\n                param_name_stream__.str(std::string());\n                param_name_stream__ << \"v\" << '.' << k_0__ << '.' << k_1__;\n                param_names__.push_back(param_name_stream__.str());\n            }\n        }\n        param_name_stream__.str(std::string());\n        param_name_stream__ << \"var_u\";\n        param_names__.push_back(param_name_stream__.str());\n        for (int k_1__ = 1; k_1__ <= (d - 1); ++k_1__) {\n            for (int k_0__ = 1; k_0__ <= 2; ++k_0__) {\n                param_name_stream__.str(std::string());\n                param_name_stream__ << \"beta\" << '.' << k_0__ << '.' << k_1__;\n                param_names__.push_back(param_name_stream__.str());\n            }\n        }\n        for (int k_0__ = 1; k_0__ <= n; ++k_0__) {\n            param_name_stream__.str(std::string());\n            param_name_stream__ << \"u\" << '.' << k_0__;\n            param_names__.push_back(param_name_stream__.str());\n        }\n\n        if (!include_gqs__ && !include_tparams__) return;\n\n        if (include_tparams__) {\n            for (int k_1__ = 1; k_1__ <= (d - 1); ++k_1__) {\n                for (int k_0__ = 1; k_0__ <= (2 * n); ++k_0__) {\n                    param_name_stream__.str(std::string());\n                    param_name_stream__ << \"mu\" << '.' << k_0__ << '.' << k_1__;\n                    param_names__.push_back(param_name_stream__.str());\n                }\n            }\n            for (int k_1__ = 1; k_1__ <= d; ++k_1__) {\n                for (int k_0__ = 1; k_0__ <= (2 * n); ++k_0__) {\n                    param_name_stream__.str(std::string());\n                    param_name_stream__ << \"theta\" << '.' << k_0__ << '.' << k_1__;\n                    param_names__.push_back(param_name_stream__.str());\n                }\n            }\n            for (int k_1__ = 1; k_1__ <= d; ++k_1__) {\n                for (int k_0__ = 1; k_0__ <= (2 * n); ++k_0__) {\n                    param_name_stream__.str(std::string());\n                    param_name_stream__ << \"full_v\" << '.' << k_0__ << '.' << k_1__;\n                    param_names__.push_back(param_name_stream__.str());\n                }\n            }\n        }\n\n\n        if (!include_gqs__) return;\n    }\n\n\n    void unconstrained_param_names(std::vector<std::string>& param_names__,\n                                   bool include_tparams__ = true,\n                                   bool include_gqs__ = true) const {\n        std::stringstream param_name_stream__;\n        for (int k_0__ = 1; k_0__ <= ((d - 1) + (((d - 1) * ((d - 1) - 1)) / 2)); ++k_0__) {\n            param_name_stream__.str(std::string());\n            param_name_stream__ << \"Sigma\" << '.' << k_0__;\n            param_names__.push_back(param_name_stream__.str());\n        }\n        for (int k_1__ = 1; k_1__ <= (d - 1); ++k_1__) {\n            for (int k_0__ = 1; k_0__ <= (2 * n); ++k_0__) {\n                param_name_stream__.str(std::string());\n                param_name_stream__ << \"v\" << '.' << k_0__ << '.' << k_1__;\n                param_names__.push_back(param_name_stream__.str());\n            }\n        }\n        param_name_stream__.str(std::string());\n        param_name_stream__ << \"var_u\";\n        param_names__.push_back(param_name_stream__.str());\n        for (int k_1__ = 1; k_1__ <= (d - 1); ++k_1__) {\n            for (int k_0__ = 1; k_0__ <= 2; ++k_0__) {\n                param_name_stream__.str(std::string());\n                param_name_stream__ << \"beta\" << '.' << k_0__ << '.' << k_1__;\n                param_names__.push_back(param_name_stream__.str());\n            }\n        }\n        for (int k_0__ = 1; k_0__ <= n; ++k_0__) {\n            param_name_stream__.str(std::string());\n            param_name_stream__ << \"u\" << '.' << k_0__;\n            param_names__.push_back(param_name_stream__.str());\n        }\n\n        if (!include_gqs__ && !include_tparams__) return;\n\n        if (include_tparams__) {\n            for (int k_1__ = 1; k_1__ <= (d - 1); ++k_1__) {\n                for (int k_0__ = 1; k_0__ <= (2 * n); ++k_0__) {\n                    param_name_stream__.str(std::string());\n                    param_name_stream__ << \"mu\" << '.' << k_0__ << '.' << k_1__;\n                    param_names__.push_back(param_name_stream__.str());\n                }\n            }\n            for (int k_1__ = 1; k_1__ <= (d - 1); ++k_1__) {\n                for (int k_0__ = 1; k_0__ <= (2 * n); ++k_0__) {\n                    param_name_stream__.str(std::string());\n                    param_name_stream__ << \"theta\" << '.' << k_0__ << '.' << k_1__;\n                    param_names__.push_back(param_name_stream__.str());\n                }\n            }\n            for (int k_1__ = 1; k_1__ <= d; ++k_1__) {\n                for (int k_0__ = 1; k_0__ <= (2 * n); ++k_0__) {\n                    param_name_stream__.str(std::string());\n                    param_name_stream__ << \"full_v\" << '.' << k_0__ << '.' << k_1__;\n                    param_names__.push_back(param_name_stream__.str());\n                }\n            }\n        }\n\n\n        if (!include_gqs__) return;\n    }\n\n}; // model\n\n}\n\ntypedef model1fb34e323adb_stan_LNM_ME_namespace::model1fb34e323adb_stan_LNM_ME stan_model;\n\n"

$model_name
[1] "stan_LNM_ME"

$model_code
[1] "\ndata{\n  int<lower=1> d; // number of compositions\n  int<lower=1> n; // number of samples\n  int w[2*n,d]; // this is the matrix of observations, which are counts (or count-like)\n  matrix[2, 2*n] x; // predictor\n  matrix[n,2*n] Z; // random effects id\n}\n\n\nparameters{\n  cov_matrix[d-1] Sigma; // covariance matrix for the normal\n  vector[d-1] v[2*n]; // a realisation of the multivariate normal\n  real<lower=0> var_u; // variance for random effects\n  matrix[2,d-1] beta; // FE coefficients\n  vector[n] u; // RE coeffcicients\n}\n\ntransformed parameters{\n  matrix[2*n,d-1] mu; // parameter for the normal. Unconstrained as it's a logR of two parts\n  simplex[d] theta[2*n]; // this is the parameter in the simplex that is fed into the multinomial\n  vector[d] full_v[2*n]; // transformed version of v, with an extra 0 for the last column\n\n  for(i in 1:2*n){\n    for(j in 1:(d-1)){\n      full_v[i,j] = v[i,j];\n    }\n    full_v[i,d] = 0;\n  }\n\n  for(l in 1:2*n){\n    theta[l,] = softmax(full_v[l,]);\n  }\n\n  mu = x'*beta + Z'*rep_matrix(u, d-1);\n\n}\n\nmodel {\n  // prior for variance of random effects\n  var_u ~ gamma(5, 5);\n  \n  // prior for random effects\n  u ~ normal(0, sqrt(var_u));\n\n  for(d_it in 1:(d-1)){\n    beta[,d_it] ~ uniform(-5, 5);\n  }\n\n  for(i in 1:2*n){\n    v[i,] ~ multi_normal(mu[i,], Sigma);\n  }\n  \n  for(i in 1:2*n){\n    w[i,] ~ multinomial(theta[i,]);\n  }\n  \n}"
attr(,"model_name2")
[1] "stan_LNM_ME"

[1] 32

SAMPLING FOR MODEL 'stan_LNM_ME' NOW (CHAIN 1).

SAMPLING FOR MODEL 'stan_LNM_ME' NOW (CHAIN 2).
Chain 2: 
Chain 2: Gradient evaluation took 0 seconds
Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
Chain 2: Adjust your expectations accordingly!
Chain 2: 
Chain 2: 
Chain 1: 
Chain 1: Gradient evaluation took 0 seconds
Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
Chain 1: Adjust your expectations accordingly!
Chain 1: 
Chain 1: 
Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 1: 
Chain 1:  Elapsed Time: 21.44 seconds (Warm-up)
Chain 1:                35.23 seconds (Sampling)
Chain 1:                56.67 seconds (Total)
Chain 1: 

SAMPLING FOR MODEL 'stan_LNM_ME' NOW (CHAIN 3).
Chain 3: 
Chain 3: Gradient evaluation took 0 seconds
Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
Chain 3: Adjust your expectations accordingly!
Chain 3: 
Chain 3: 
Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 2: 
Chain 2:  Elapsed Time: 41.34 seconds (Warm-up)
Chain 2:                105.05 seconds (Sampling)
Chain 2:                146.39 seconds (Total)
Chain 2: 

SAMPLING FOR MODEL 'stan_LNM_ME' NOW (CHAIN 4).
Chain 4: 
Chain 4: Gradient evaluation took 0 seconds
Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0 seconds.
Chain 4: Adjust your expectations accordingly!
Chain 4: 
Chain 4: 
Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 3: 
Chain 3:  Elapsed Time: 46.57 seconds (Warm-up)
Chain 3:                55.77 seconds (Sampling)
Chain 3:                102.34 seconds (Total)
Chain 3: 
Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 4: 
Chain 4:  Elapsed Time: 31.83 seconds (Warm-up)
Chain 4:                34.61 seconds (Sampling)
Chain 4:                66.44 seconds (Total)
Chain 4: 
Warning messages:
1: There were 3995 divergent transitions after warmup. Increasing adapt_delta above 0.8 may help. See
http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup 
2: There were 5 transitions after warmup that exceeded the maximum treedepth. Increase max_treedepth above 10. See
http://mc-stan.org/misc/warnings.html#maximum-treedepth-exceeded 
3: Examine the pairs() plot to diagnose sampling problems
 
[Sun Aug  2 16:57:18 2020]
Finished job 0.
1 of 1 steps (100%) done
