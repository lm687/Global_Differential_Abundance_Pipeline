Loading required package: StanHeaders
Loading required package: ggplot2
rstan (Version 2.19.3, GitRev: 2e1f913d3ca3)
For execution on a local, multicore CPU with excess RAM we recommend calling
options(mc.cores = parallel::detectCores()).
To avoid recompilation of unchanged Stan programs, we recommend calling
rstan_options(auto_write = TRUE)
Loading required package: MASS
$status
[1] TRUE

$model_cppname
[1] "model162b0333519e1_stan_DM_ME"

$cppcode
[1] "// Code generated by Stan version 2.21.0\n\n#include <stan/model/model_header.hpp>\n\nnamespace model162b0333519e1_stan_DM_ME_namespace {\n\nusing std::istream;\nusing std::string;\nusing std::stringstream;\nusing std::vector;\nusing stan::io::dump;\nusing stan::math::lgamma;\nusing stan::model::prob_grad;\nusing namespace stan::math;\n\nstatic int current_statement_begin__;\n\nstan::io::program_reader prog_reader__() {\n    stan::io::program_reader reader;\n    reader.add_event(0, 0, \"start\", \"model162b0333519e1_stan_DM_ME\");\n    reader.add_event(57, 55, \"end\", \"model162b0333519e1_stan_DM_ME\");\n    return reader;\n}\n\ntemplate <bool propto, typename T1__>\ntypename boost::math::tools::promote_args<T1__>::type\ndirichlet_multinomial_lpmf(const std::vector<int>& y,\n                               const Eigen::Matrix<T1__, Eigen::Dynamic, 1>& alpha, std::ostream* pstream__) {\n    typedef typename boost::math::tools::promote_args<T1__>::type local_scalar_t__;\n    typedef local_scalar_t__ fun_return_scalar_t__;\n    const static bool propto__ = true;\n    (void) propto__;\n        local_scalar_t__ DUMMY_VAR__(std::numeric_limits<double>::quiet_NaN());\n        (void) DUMMY_VAR__;  // suppress unused var warning\n\n    int current_statement_begin__ = -1;\n    try {\n        {\n        current_statement_begin__ = 4;\n        local_scalar_t__ alpha_plus(DUMMY_VAR__);\n        (void) alpha_plus;  // dummy to suppress unused var warning\n        stan::math::initialize(alpha_plus, DUMMY_VAR__);\n        stan::math::fill(alpha_plus, DUMMY_VAR__);\n        stan::math::assign(alpha_plus,sum(alpha));\n\n\n        current_statement_begin__ = 5;\n        return stan::math::promote_scalar<fun_return_scalar_t__>((((stan::math::lgamma(alpha_plus) + sum(stan::math::lgamma(add(alpha, to_vector(y))))) - stan::math::lgamma((alpha_plus + sum(y)))) - sum(stan::math::lgamma(alpha))));\n        }\n    } catch (const std::exception& e) {\n        stan::lang::rethrow_located(e, current_statement_begin__, prog_reader__());\n        // Next line prevents compiler griping about no return\n        throw std::runtime_error(\"*** IF YOU SEE THIS, PLEASE REPORT A BUG ***\");\n    }\n}\ntemplate <typename T1__>\ntypename boost::math::tools::promote_args<T1__>::type\ndirichlet_multinomial_lpmf(const std::vector<int>& y,\n                               const Eigen::Matrix<T1__, Eigen::Dynamic, 1>& alpha, std::ostream* pstream__) {\n    return dirichlet_multinomial_lpmf<false>(y,alpha, pstream__);\n}\n\n\nstruct dirichlet_multinomial_lpmf_functor__ {\n    template <bool propto, typename T1__>\n        typename boost::math::tools::promote_args<T1__>::type\n    operator()(const std::vector<int>& y,\n                               const Eigen::Matrix<T1__, Eigen::Dynamic, 1>& alpha, std::ostream* pstream__) const {\n        return dirichlet_multinomial_lpmf(y, alpha, pstream__);\n    }\n};\n\nclass model162b0333519e1_stan_DM_ME\n  : public stan::model::model_base_crtp<model162b0333519e1_stan_DM_ME> {\nprivate:\n        int n;\n        int d;\n        std::vector<std::vector<int> > w;\n        int p;\n        matrix_d x;\n        matrix_d Z;\npublic:\n    model162b0333519e1_stan_DM_ME(stan::io::var_context& context__,\n        std::ostream* pstream__ = 0)\n        : model_base_crtp(0) {\n        ctor_body(context__, 0, pstream__);\n    }\n\n    model162b0333519e1_stan_DM_ME(stan::io::var_context& context__,\n        unsigned int random_seed__,\n        std::ostream* pstream__ = 0)\n        : model_base_crtp(0) {\n        ctor_body(context__, random_seed__, pstream__);\n    }\n\n    void ctor_body(stan::io::var_context& context__,\n                   unsigned int random_seed__,\n                   std::ostream* pstream__) {\n        typedef double local_scalar_t__;\n\n        boost::ecuyer1988 base_rng__ =\n          stan::services::util::create_rng(random_seed__, 0);\n        (void) base_rng__;  // suppress unused var warning\n\n        current_statement_begin__ = -1;\n\n        static const char* function__ = \"model162b0333519e1_stan_DM_ME_namespace::model162b0333519e1_stan_DM_ME\";\n        (void) function__;  // dummy to suppress unused var warning\n        size_t pos__;\n        (void) pos__;  // dummy to suppress unused var warning\n        std::vector<int> vals_i__;\n        std::vector<double> vals_r__;\n        local_scalar_t__ DUMMY_VAR__(std::numeric_limits<double>::quiet_NaN());\n        (void) DUMMY_VAR__;  // suppress unused var warning\n\n        try {\n            // initialize data block variables from context__\n            current_statement_begin__ = 11;\n            context__.validate_dims(\"data initialization\", \"n\", \"int\", context__.to_vec());\n            n = int(0);\n            vals_i__ = context__.vals_i(\"n\");\n            pos__ = 0;\n            n = vals_i__[pos__++];\n            check_greater_or_equal(function__, \"n\", n, 0);\n\n            current_statement_begin__ = 12;\n            context__.validate_dims(\"data initialization\", \"d\", \"int\", context__.to_vec());\n            d = int(0);\n            vals_i__ = context__.vals_i(\"d\");\n            pos__ = 0;\n            d = vals_i__[pos__++];\n            check_greater_or_equal(function__, \"d\", d, 0);\n\n            current_statement_begin__ = 13;\n            validate_non_negative_index(\"w\", \"(2 * n)\", (2 * n));\n            validate_non_negative_index(\"w\", \"d\", d);\n            context__.validate_dims(\"data initialization\", \"w\", \"int\", context__.to_vec((2 * n),d));\n            w = std::vector<std::vector<int> >((2 * n), std::vector<int>(d, int(0)));\n            vals_i__ = context__.vals_i(\"w\");\n            pos__ = 0;\n            size_t w_k_0_max__ = (2 * n);\n            size_t w_k_1_max__ = d;\n            for (size_t k_1__ = 0; k_1__ < w_k_1_max__; ++k_1__) {\n                for (size_t k_0__ = 0; k_0__ < w_k_0_max__; ++k_0__) {\n                    w[k_0__][k_1__] = vals_i__[pos__++];\n                }\n            }\n\n            current_statement_begin__ = 14;\n            context__.validate_dims(\"data initialization\", \"p\", \"int\", context__.to_vec());\n            p = int(0);\n            vals_i__ = context__.vals_i(\"p\");\n            pos__ = 0;\n            p = vals_i__[pos__++];\n\n            current_statement_begin__ = 15;\n            validate_non_negative_index(\"x\", \"p\", p);\n            validate_non_negative_index(\"x\", \"(2 * n)\", (2 * n));\n            context__.validate_dims(\"data initialization\", \"x\", \"matrix_d\", context__.to_vec(p,(2 * n)));\n            x = Eigen::Matrix<double, Eigen::Dynamic, Eigen::Dynamic>(p, (2 * n));\n            vals_r__ = context__.vals_r(\"x\");\n            pos__ = 0;\n            size_t x_j_2_max__ = (2 * n);\n            size_t x_j_1_max__ = p;\n            for (size_t j_2__ = 0; j_2__ < x_j_2_max__; ++j_2__) {\n                for (size_t j_1__ = 0; j_1__ < x_j_1_max__; ++j_1__) {\n                    x(j_1__, j_2__) = vals_r__[pos__++];\n                }\n            }\n\n            current_statement_begin__ = 16;\n            validate_non_negative_index(\"Z\", \"n\", n);\n            validate_non_negative_index(\"Z\", \"(2 * n)\", (2 * n));\n            context__.validate_dims(\"data initialization\", \"Z\", \"matrix_d\", context__.to_vec(n,(2 * n)));\n            Z = Eigen::Matrix<double, Eigen::Dynamic, Eigen::Dynamic>(n, (2 * n));\n            vals_r__ = context__.vals_r(\"Z\");\n            pos__ = 0;\n            size_t Z_j_2_max__ = (2 * n);\n            size_t Z_j_1_max__ = n;\n            for (size_t j_2__ = 0; j_2__ < Z_j_2_max__; ++j_2__) {\n                for (size_t j_1__ = 0; j_1__ < Z_j_1_max__; ++j_1__) {\n                    Z(j_1__, j_2__) = vals_r__[pos__++];\n                }\n            }\n\n\n            // initialize transformed data variables\n            // execute transformed data statements\n\n            // validate transformed data\n\n            // validate, set parameter ranges\n            num_params_r__ = 0U;\n            param_ranges_i__.clear();\n            current_statement_begin__ = 20;\n            validate_non_negative_index(\"beta\", \"p\", p);\n            validate_non_negative_index(\"beta\", \"(d - 1)\", (d - 1));\n            num_params_r__ += (p * (d - 1));\n            current_statement_begin__ = 21;\n            validate_non_negative_index(\"u\", \"n\", n);\n            num_params_r__ += n;\n            current_statement_begin__ = 22;\n            num_params_r__ += 1;\n            current_statement_begin__ = 23;\n            validate_non_negative_index(\"overdispersion_scalar\", \"2\", 2);\n            num_params_r__ += (1 * 2);\n        } catch (const std::exception& e) {\n            stan::lang::rethrow_located(e, current_statement_begin__, prog_reader__());\n            // Next line prevents compiler griping about no return\n            throw std::runtime_error(\"*** IF YOU SEE THIS, PLEASE REPORT A BUG ***\");\n        }\n    }\n\n    ~model162b0333519e1_stan_DM_ME() { }\n\n\n    void transform_inits(const stan::io::var_context& context__,\n                         std::vector<int>& params_i__,\n                         std::vector<double>& params_r__,\n                         std::ostream* pstream__) const {\n        typedef double local_scalar_t__;\n        stan::io::writer<double> writer__(params_r__, params_i__);\n        size_t pos__;\n        (void) pos__; // dummy call to supress warning\n        std::vector<double> vals_r__;\n        std::vector<int> vals_i__;\n\n        current_statement_begin__ = 20;\n        if (!(context__.contains_r(\"beta\")))\n            stan::lang::rethrow_located(std::runtime_error(std::string(\"Variable beta missing\")), current_statement_begin__, prog_reader__());\n        vals_r__ = context__.vals_r(\"beta\");\n        pos__ = 0U;\n        validate_non_negative_index(\"beta\", \"p\", p);\n        validate_non_negative_index(\"beta\", \"(d - 1)\", (d - 1));\n        context__.validate_dims(\"parameter initialization\", \"beta\", \"matrix_d\", context__.to_vec(p,(d - 1)));\n        Eigen::Matrix<double, Eigen::Dynamic, Eigen::Dynamic> beta(p, (d - 1));\n        size_t beta_j_2_max__ = (d - 1);\n        size_t beta_j_1_max__ = p;\n        for (size_t j_2__ = 0; j_2__ < beta_j_2_max__; ++j_2__) {\n            for (size_t j_1__ = 0; j_1__ < beta_j_1_max__; ++j_1__) {\n                beta(j_1__, j_2__) = vals_r__[pos__++];\n            }\n        }\n        try {\n            writer__.matrix_unconstrain(beta);\n        } catch (const std::exception& e) {\n            stan::lang::rethrow_located(std::runtime_error(std::string(\"Error transforming variable beta: \") + e.what()), current_statement_begin__, prog_reader__());\n        }\n\n        current_statement_begin__ = 21;\n        if (!(context__.contains_r(\"u\")))\n            stan::lang::rethrow_located(std::runtime_error(std::string(\"Variable u missing\")), current_statement_begin__, prog_reader__());\n        vals_r__ = context__.vals_r(\"u\");\n        pos__ = 0U;\n        validate_non_negative_index(\"u\", \"n\", n);\n        context__.validate_dims(\"parameter initialization\", \"u\", \"vector_d\", context__.to_vec(n));\n        Eigen::Matrix<double, Eigen::Dynamic, 1> u(n);\n        size_t u_j_1_max__ = n;\n        for (size_t j_1__ = 0; j_1__ < u_j_1_max__; ++j_1__) {\n            u(j_1__) = vals_r__[pos__++];\n        }\n        try {\n            writer__.vector_unconstrain(u);\n        } catch (const std::exception& e) {\n            stan::lang::rethrow_located(std::runtime_error(std::string(\"Error transforming variable u: \") + e.what()), current_statement_begin__, prog_reader__());\n        }\n\n        current_statement_begin__ = 22;\n        if (!(context__.contains_r(\"sigma_u\")))\n            stan::lang::rethrow_located(std::runtime_error(std::string(\"Variable sigma_u missing\")), current_statement_begin__, prog_reader__());\n        vals_r__ = context__.vals_r(\"sigma_u\");\n        pos__ = 0U;\n        context__.validate_dims(\"parameter initialization\", \"sigma_u\", \"double\", context__.to_vec());\n        double sigma_u(0);\n        sigma_u = vals_r__[pos__++];\n        try {\n            writer__.scalar_lb_unconstrain(0, sigma_u);\n        } catch (const std::exception& e) {\n            stan::lang::rethrow_located(std::runtime_error(std::string(\"Error transforming variable sigma_u: \") + e.what()), current_statement_begin__, prog_reader__());\n        }\n\n        current_statement_begin__ = 23;\n        if (!(context__.contains_r(\"overdispersion_scalar\")))\n            stan::lang::rethrow_located(std::runtime_error(std::string(\"Variable overdispersion_scalar missing\")), current_statement_begin__, prog_reader__());\n        vals_r__ = context__.vals_r(\"overdispersion_scalar\");\n        pos__ = 0U;\n        validate_non_negative_index(\"overdispersion_scalar\", \"2\", 2);\n        context__.validate_dims(\"parameter initialization\", \"overdispersion_scalar\", \"double\", context__.to_vec(2));\n        std::vector<double> overdispersion_scalar(2, double(0));\n        size_t overdispersion_scalar_k_0_max__ = 2;\n        for (size_t k_0__ = 0; k_0__ < overdispersion_scalar_k_0_max__; ++k_0__) {\n            overdispersion_scalar[k_0__] = vals_r__[pos__++];\n        }\n        size_t overdispersion_scalar_i_0_max__ = 2;\n        for (size_t i_0__ = 0; i_0__ < overdispersion_scalar_i_0_max__; ++i_0__) {\n            try {\n                writer__.scalar_lb_unconstrain(0, overdispersion_scalar[i_0__]);\n            } catch (const std::exception& e) {\n                stan::lang::rethrow_located(std::runtime_error(std::string(\"Error transforming variable overdispersion_scalar: \") + e.what()), current_statement_begin__, prog_reader__());\n            }\n        }\n\n        params_r__ = writer__.data_r();\n        params_i__ = writer__.data_i();\n    }\n\n    void transform_inits(const stan::io::var_context& context,\n                         Eigen::Matrix<double, Eigen::Dynamic, 1>& params_r,\n                         std::ostream* pstream__) const {\n      std::vector<double> params_r_vec;\n      std::vector<int> params_i_vec;\n      transform_inits(context, params_i_vec, params_r_vec, pstream__);\n      params_r.resize(params_r_vec.size());\n      for (int i = 0; i < params_r.size(); ++i)\n        params_r(i) = params_r_vec[i];\n    }\n\n\n    template <bool propto__, bool jacobian__, typename T__>\n    T__ log_prob(std::vector<T__>& params_r__,\n                 std::vector<int>& params_i__,\n                 std::ostream* pstream__ = 0) const {\n\n        typedef T__ local_scalar_t__;\n\n        local_scalar_t__ DUMMY_VAR__(std::numeric_limits<double>::quiet_NaN());\n        (void) DUMMY_VAR__;  // dummy to suppress unused var warning\n\n        T__ lp__(0.0);\n        stan::math::accumulator<T__> lp_accum__;\n        try {\n            stan::io::reader<local_scalar_t__> in__(params_r__, params_i__);\n\n            // model parameters\n            current_statement_begin__ = 20;\n            Eigen::Matrix<local_scalar_t__, Eigen::Dynamic, Eigen::Dynamic> beta;\n            (void) beta;  // dummy to suppress unused var warning\n            if (jacobian__)\n                beta = in__.matrix_constrain(p, (d - 1), lp__);\n            else\n                beta = in__.matrix_constrain(p, (d - 1));\n\n            current_statement_begin__ = 21;\n            Eigen::Matrix<local_scalar_t__, Eigen::Dynamic, 1> u;\n            (void) u;  // dummy to suppress unused var warning\n            if (jacobian__)\n                u = in__.vector_constrain(n, lp__);\n            else\n                u = in__.vector_constrain(n);\n\n            current_statement_begin__ = 22;\n            local_scalar_t__ sigma_u;\n            (void) sigma_u;  // dummy to suppress unused var warning\n            if (jacobian__)\n                sigma_u = in__.scalar_lb_constrain(0, lp__);\n            else\n                sigma_u = in__.scalar_lb_constrain(0);\n\n            current_statement_begin__ = 23;\n            std::vector<local_scalar_t__> overdispersion_scalar;\n            size_t overdispersion_scalar_d_0_max__ = 2;\n            overdispersion_scalar.reserve(overdispersion_scalar_d_0_max__);\n            for (size_t d_0__ = 0; d_0__ < overdispersion_scalar_d_0_max__; ++d_0__) {\n                if (jacobian__)\n                    overdispersion_scalar.push_back(in__.scalar_lb_constrain(0, lp__));\n                else\n                    overdispersion_scalar.push_back(in__.scalar_lb_constrain(0));\n            }\n\n            // transformed parameters\n            current_statement_begin__ = 27;\n            validate_non_negative_index(\"alpha_mean\", \"(2 * n)\", (2 * n));\n            validate_non_negative_index(\"alpha_mean\", \"d\", d);\n            Eigen::Matrix<local_scalar_t__, Eigen::Dynamic, Eigen::Dynamic> alpha_mean((2 * n), d);\n            stan::math::initialize(alpha_mean, DUMMY_VAR__);\n            stan::math::fill(alpha_mean, DUMMY_VAR__);\n\n            current_statement_begin__ = 28;\n            validate_non_negative_index(\"alpha\", \"(2 * n)\", (2 * n));\n            validate_non_negative_index(\"alpha\", \"d\", d);\n            Eigen::Matrix<local_scalar_t__, Eigen::Dynamic, Eigen::Dynamic> alpha((2 * n), d);\n            stan::math::initialize(alpha, DUMMY_VAR__);\n            stan::math::fill(alpha, DUMMY_VAR__);\n\n            current_statement_begin__ = 29;\n            validate_non_negative_index(\"overdispersion_scalars\", \"(2 * n)\", (2 * n));\n            Eigen::Matrix<local_scalar_t__, Eigen::Dynamic, 1> overdispersion_scalars((2 * n));\n            stan::math::initialize(overdispersion_scalars, DUMMY_VAR__);\n            stan::math::fill(overdispersion_scalars, DUMMY_VAR__);\n            stan::math::assign(overdispersion_scalars,append_row(rep_vector(get_base1(overdispersion_scalar, 1, \"overdispersion_scalar\", 1), n), rep_vector(get_base1(overdispersion_scalar, 2, \"overdispersion_scalar\", 1), n)));\n\n            // transformed parameters block statements\n            current_statement_begin__ = 31;\n            stan::math::assign(alpha_mean, append_col(add(multiply(transpose(x), beta), multiply(transpose(Z), rep_matrix(u, (d - 1)))), rep_vector(0, (2 * n))));\n            current_statement_begin__ = 32;\n            for (int l = 1; l <= (2 * n); ++l) {\n\n                current_statement_begin__ = 33;\n                stan::model::assign(alpha, \n                            stan::model::cons_list(stan::model::index_uni(l), stan::model::cons_list(stan::model::index_omni(), stan::model::nil_index_list())), \n                            multiply(to_row_vector(softmax(to_vector(stan::model::rvalue(alpha_mean, stan::model::cons_list(stan::model::index_uni(l), stan::model::cons_list(stan::model::index_omni(), stan::model::nil_index_list())), \"alpha_mean\")))), get_base1(overdispersion_scalars, l, \"overdispersion_scalars\", 1)), \n                            \"assigning variable alpha\");\n            }\n\n            // validate transformed parameters\n            const char* function__ = \"validate transformed params\";\n            (void) function__;  // dummy to suppress unused var warning\n\n            current_statement_begin__ = 27;\n            size_t alpha_mean_j_1_max__ = (2 * n);\n            size_t alpha_mean_j_2_max__ = d;\n            for (size_t j_1__ = 0; j_1__ < alpha_mean_j_1_max__; ++j_1__) {\n                for (size_t j_2__ = 0; j_2__ < alpha_mean_j_2_max__; ++j_2__) {\n                    if (stan::math::is_uninitialized(alpha_mean(j_1__, j_2__))) {\n                        std::stringstream msg__;\n                        msg__ << \"Undefined transformed parameter: alpha_mean\" << \"(\" << j_1__ << \", \" << j_2__ << \")\";\n                        stan::lang::rethrow_located(std::runtime_error(std::string(\"Error initializing variable alpha_mean: \") + msg__.str()), current_statement_begin__, prog_reader__());\n                    }\n                }\n            }\n            current_statement_begin__ = 28;\n            size_t alpha_j_1_max__ = (2 * n);\n            size_t alpha_j_2_max__ = d;\n            for (size_t j_1__ = 0; j_1__ < alpha_j_1_max__; ++j_1__) {\n                for (size_t j_2__ = 0; j_2__ < alpha_j_2_max__; ++j_2__) {\n                    if (stan::math::is_uninitialized(alpha(j_1__, j_2__))) {\n                        std::stringstream msg__;\n                        msg__ << \"Undefined transformed parameter: alpha\" << \"(\" << j_1__ << \", \" << j_2__ << \")\";\n                        stan::lang::rethrow_located(std::runtime_error(std::string(\"Error initializing variable alpha: \") + msg__.str()), current_statement_begin__, prog_reader__());\n                    }\n                }\n            }\n            check_greater_or_equal(function__, \"alpha\", alpha, 0);\n\n            current_statement_begin__ = 29;\n            size_t overdispersion_scalars_j_1_max__ = (2 * n);\n            for (size_t j_1__ = 0; j_1__ < overdispersion_scalars_j_1_max__; ++j_1__) {\n                if (stan::math::is_uninitialized(overdispersion_scalars(j_1__))) {\n                    std::stringstream msg__;\n                    msg__ << \"Undefined transformed parameter: overdispersion_scalars\" << \"(\" << j_1__ << \")\";\n                    stan::lang::rethrow_located(std::runtime_error(std::string(\"Error initializing variable overdispersion_scalars: \") + msg__.str()), current_statement_begin__, prog_reader__());\n                }\n            }\n            check_greater_or_equal(function__, \"overdispersion_scalars\", overdispersion_scalars, 0);\n\n\n            // model body\n\n            current_statement_begin__ = 39;\n            lp_accum__.add(gamma_log<propto__>(sigma_u, 5, 5));\n            current_statement_begin__ = 41;\n            lp_accum__.add(normal_log<propto__>(overdispersion_scalar, 1, 1));\n            current_statement_begin__ = 44;\n            for (int d_it = 1; d_it <= (d - 1); ++d_it) {\n\n                current_statement_begin__ = 45;\n                lp_accum__.add(uniform_log<propto__>(stan::model::rvalue(beta, stan::model::cons_list(stan::model::index_omni(), stan::model::cons_list(stan::model::index_uni(d_it), stan::model::nil_index_list())), \"beta\"), -(5), 5));\n            }\n            current_statement_begin__ = 49;\n            lp_accum__.add(normal_log<propto__>(to_vector(u), 0, stan::math::sqrt(sigma_u)));\n            current_statement_begin__ = 51;\n            for (int l = 1; l <= (2 * n); ++l) {\n\n                current_statement_begin__ = 52;\n                lp_accum__.add(dirichlet_multinomial_lpmf<propto__>(stan::model::rvalue(w, stan::model::cons_list(stan::model::index_uni(l), stan::model::cons_list(stan::model::index_omni(), stan::model::nil_index_list())), \"w\"), to_vector(stan::model::rvalue(alpha, stan::model::cons_list(stan::model::index_uni(l), stan::model::cons_list(stan::model::index_omni(), stan::model::nil_index_list())), \"alpha\")), pstream__));\n            }\n\n        } catch (const std::exception& e) {\n            stan::lang::rethrow_located(e, current_statement_begin__, prog_reader__());\n            // Next line prevents compiler griping about no return\n            throw std::runtime_error(\"*** IF YOU SEE THIS, PLEASE REPORT A BUG ***\");\n        }\n\n        lp_accum__.add(lp__);\n        return lp_accum__.sum();\n\n    } // log_prob()\n\n    template <bool propto, bool jacobian, typename T_>\n    T_ log_prob(Eigen::Matrix<T_,Eigen::Dynamic,1>& params_r,\n               std::ostream* pstream = 0) const {\n      std::vector<T_> vec_params_r;\n      vec_params_r.reserve(params_r.size());\n      for (int i = 0; i < params_r.size(); ++i)\n        vec_params_r.push_back(params_r(i));\n      std::vector<int> vec_params_i;\n      return log_prob<propto,jacobian,T_>(vec_params_r, vec_params_i, pstream);\n    }\n\n\n    void get_param_names(std::vector<std::string>& names__) const {\n        names__.resize(0);\n        names__.push_back(\"beta\");\n        names__.push_back(\"u\");\n        names__.push_back(\"sigma_u\");\n        names__.push_back(\"overdispersion_scalar\");\n        names__.push_back(\"alpha_mean\");\n        names__.push_back(\"alpha\");\n        names__.push_back(\"overdispersion_scalars\");\n    }\n\n\n    void get_dims(std::vector<std::vector<size_t> >& dimss__) const {\n        dimss__.resize(0);\n        std::vector<size_t> dims__;\n        dims__.resize(0);\n        dims__.push_back(p);\n        dims__.push_back((d - 1));\n        dimss__.push_back(dims__);\n        dims__.resize(0);\n        dims__.push_back(n);\n        dimss__.push_back(dims__);\n        dims__.resize(0);\n        dimss__.push_back(dims__);\n        dims__.resize(0);\n        dims__.push_back(2);\n        dimss__.push_back(dims__);\n        dims__.resize(0);\n        dims__.push_back((2 * n));\n        dims__.push_back(d);\n        dimss__.push_back(dims__);\n        dims__.resize(0);\n        dims__.push_back((2 * n));\n        dims__.push_back(d);\n        dimss__.push_back(dims__);\n        dims__.resize(0);\n        dims__.push_back((2 * n));\n        dimss__.push_back(dims__);\n    }\n\n    template <typename RNG>\n    void write_array(RNG& base_rng__,\n                     std::vector<double>& params_r__,\n                     std::vector<int>& params_i__,\n                     std::vector<double>& vars__,\n                     bool include_tparams__ = true,\n                     bool include_gqs__ = true,\n                     std::ostream* pstream__ = 0) const {\n        typedef double local_scalar_t__;\n\n        vars__.resize(0);\n        stan::io::reader<local_scalar_t__> in__(params_r__, params_i__);\n        static const char* function__ = \"model162b0333519e1_stan_DM_ME_namespace::write_array\";\n        (void) function__;  // dummy to suppress unused var warning\n\n        // read-transform, write parameters\n        Eigen::Matrix<double, Eigen::Dynamic, Eigen::Dynamic> beta = in__.matrix_constrain(p, (d - 1));\n        size_t beta_j_2_max__ = (d - 1);\n        size_t beta_j_1_max__ = p;\n        for (size_t j_2__ = 0; j_2__ < beta_j_2_max__; ++j_2__) {\n            for (size_t j_1__ = 0; j_1__ < beta_j_1_max__; ++j_1__) {\n                vars__.push_back(beta(j_1__, j_2__));\n            }\n        }\n\n        Eigen::Matrix<double, Eigen::Dynamic, 1> u = in__.vector_constrain(n);\n        size_t u_j_1_max__ = n;\n        for (size_t j_1__ = 0; j_1__ < u_j_1_max__; ++j_1__) {\n            vars__.push_back(u(j_1__));\n        }\n\n        double sigma_u = in__.scalar_lb_constrain(0);\n        vars__.push_back(sigma_u);\n\n        std::vector<double> overdispersion_scalar;\n        size_t overdispersion_scalar_d_0_max__ = 2;\n        overdispersion_scalar.reserve(overdispersion_scalar_d_0_max__);\n        for (size_t d_0__ = 0; d_0__ < overdispersion_scalar_d_0_max__; ++d_0__) {\n            overdispersion_scalar.push_back(in__.scalar_lb_constrain(0));\n        }\n        size_t overdispersion_scalar_k_0_max__ = 2;\n        for (size_t k_0__ = 0; k_0__ < overdispersion_scalar_k_0_max__; ++k_0__) {\n            vars__.push_back(overdispersion_scalar[k_0__]);\n        }\n\n        double lp__ = 0.0;\n        (void) lp__;  // dummy to suppress unused var warning\n        stan::math::accumulator<double> lp_accum__;\n\n        local_scalar_t__ DUMMY_VAR__(std::numeric_limits<double>::quiet_NaN());\n        (void) DUMMY_VAR__;  // suppress unused var warning\n\n        if (!include_tparams__ && !include_gqs__) return;\n\n        try {\n            // declare and define transformed parameters\n            current_statement_begin__ = 27;\n            validate_non_negative_index(\"alpha_mean\", \"(2 * n)\", (2 * n));\n            validate_non_negative_index(\"alpha_mean\", \"d\", d);\n            Eigen::Matrix<double, Eigen::Dynamic, Eigen::Dynamic> alpha_mean((2 * n), d);\n            stan::math::initialize(alpha_mean, DUMMY_VAR__);\n            stan::math::fill(alpha_mean, DUMMY_VAR__);\n\n            current_statement_begin__ = 28;\n            validate_non_negative_index(\"alpha\", \"(2 * n)\", (2 * n));\n            validate_non_negative_index(\"alpha\", \"d\", d);\n            Eigen::Matrix<double, Eigen::Dynamic, Eigen::Dynamic> alpha((2 * n), d);\n            stan::math::initialize(alpha, DUMMY_VAR__);\n            stan::math::fill(alpha, DUMMY_VAR__);\n\n            current_statement_begin__ = 29;\n            validate_non_negative_index(\"overdispersion_scalars\", \"(2 * n)\", (2 * n));\n            Eigen::Matrix<double, Eigen::Dynamic, 1> overdispersion_scalars((2 * n));\n            stan::math::initialize(overdispersion_scalars, DUMMY_VAR__);\n            stan::math::fill(overdispersion_scalars, DUMMY_VAR__);\n            stan::math::assign(overdispersion_scalars,append_row(rep_vector(get_base1(overdispersion_scalar, 1, \"overdispersion_scalar\", 1), n), rep_vector(get_base1(overdispersion_scalar, 2, \"overdispersion_scalar\", 1), n)));\n\n            // do transformed parameters statements\n            current_statement_begin__ = 31;\n            stan::math::assign(alpha_mean, append_col(add(multiply(transpose(x), beta), multiply(transpose(Z), rep_matrix(u, (d - 1)))), rep_vector(0, (2 * n))));\n            current_statement_begin__ = 32;\n            for (int l = 1; l <= (2 * n); ++l) {\n\n                current_statement_begin__ = 33;\n                stan::model::assign(alpha, \n                            stan::model::cons_list(stan::model::index_uni(l), stan::model::cons_list(stan::model::index_omni(), stan::model::nil_index_list())), \n                            multiply(to_row_vector(softmax(to_vector(stan::model::rvalue(alpha_mean, stan::model::cons_list(stan::model::index_uni(l), stan::model::cons_list(stan::model::index_omni(), stan::model::nil_index_list())), \"alpha_mean\")))), get_base1(overdispersion_scalars, l, \"overdispersion_scalars\", 1)), \n                            \"assigning variable alpha\");\n            }\n\n            if (!include_gqs__ && !include_tparams__) return;\n            // validate transformed parameters\n            const char* function__ = \"validate transformed params\";\n            (void) function__;  // dummy to suppress unused var warning\n\n            current_statement_begin__ = 28;\n            check_greater_or_equal(function__, \"alpha\", alpha, 0);\n\n            current_statement_begin__ = 29;\n            check_greater_or_equal(function__, \"overdispersion_scalars\", overdispersion_scalars, 0);\n\n            // write transformed parameters\n            if (include_tparams__) {\n                size_t alpha_mean_j_2_max__ = d;\n                size_t alpha_mean_j_1_max__ = (2 * n);\n                for (size_t j_2__ = 0; j_2__ < alpha_mean_j_2_max__; ++j_2__) {\n                    for (size_t j_1__ = 0; j_1__ < alpha_mean_j_1_max__; ++j_1__) {\n                        vars__.push_back(alpha_mean(j_1__, j_2__));\n                    }\n                }\n                size_t alpha_j_2_max__ = d;\n                size_t alpha_j_1_max__ = (2 * n);\n                for (size_t j_2__ = 0; j_2__ < alpha_j_2_max__; ++j_2__) {\n                    for (size_t j_1__ = 0; j_1__ < alpha_j_1_max__; ++j_1__) {\n                        vars__.push_back(alpha(j_1__, j_2__));\n                    }\n                }\n                size_t overdispersion_scalars_j_1_max__ = (2 * n);\n                for (size_t j_1__ = 0; j_1__ < overdispersion_scalars_j_1_max__; ++j_1__) {\n                    vars__.push_back(overdispersion_scalars(j_1__));\n                }\n            }\n            if (!include_gqs__) return;\n        } catch (const std::exception& e) {\n            stan::lang::rethrow_located(e, current_statement_begin__, prog_reader__());\n            // Next line prevents compiler griping about no return\n            throw std::runtime_error(\"*** IF YOU SEE THIS, PLEASE REPORT A BUG ***\");\n        }\n    }\n\n    template <typename RNG>\n    void write_array(RNG& base_rng,\n                     Eigen::Matrix<double,Eigen::Dynamic,1>& params_r,\n                     Eigen::Matrix<double,Eigen::Dynamic,1>& vars,\n                     bool include_tparams = true,\n                     bool include_gqs = true,\n                     std::ostream* pstream = 0) const {\n      std::vector<double> params_r_vec(params_r.size());\n      for (int i = 0; i < params_r.size(); ++i)\n        params_r_vec[i] = params_r(i);\n      std::vector<double> vars_vec;\n      std::vector<int> params_i_vec;\n      write_array(base_rng, params_r_vec, params_i_vec, vars_vec, include_tparams, include_gqs, pstream);\n      vars.resize(vars_vec.size());\n      for (int i = 0; i < vars.size(); ++i)\n        vars(i) = vars_vec[i];\n    }\n\n    std::string model_name() const {\n        return \"model162b0333519e1_stan_DM_ME\";\n    }\n\n\n    void constrained_param_names(std::vector<std::string>& param_names__,\n                                 bool include_tparams__ = true,\n                                 bool include_gqs__ = true) const {\n        std::stringstream param_name_stream__;\n        size_t beta_j_2_max__ = (d - 1);\n        size_t beta_j_1_max__ = p;\n        for (size_t j_2__ = 0; j_2__ < beta_j_2_max__; ++j_2__) {\n            for (size_t j_1__ = 0; j_1__ < beta_j_1_max__; ++j_1__) {\n                param_name_stream__.str(std::string());\n                param_name_stream__ << \"beta\" << '.' << j_1__ + 1 << '.' << j_2__ + 1;\n                param_names__.push_back(param_name_stream__.str());\n            }\n        }\n        size_t u_j_1_max__ = n;\n        for (size_t j_1__ = 0; j_1__ < u_j_1_max__; ++j_1__) {\n            param_name_stream__.str(std::string());\n            param_name_stream__ << \"u\" << '.' << j_1__ + 1;\n            param_names__.push_back(param_name_stream__.str());\n        }\n        param_name_stream__.str(std::string());\n        param_name_stream__ << \"sigma_u\";\n        param_names__.push_back(param_name_stream__.str());\n        size_t overdispersion_scalar_k_0_max__ = 2;\n        for (size_t k_0__ = 0; k_0__ < overdispersion_scalar_k_0_max__; ++k_0__) {\n            param_name_stream__.str(std::string());\n            param_name_stream__ << \"overdispersion_scalar\" << '.' << k_0__ + 1;\n            param_names__.push_back(param_name_stream__.str());\n        }\n\n        if (!include_gqs__ && !include_tparams__) return;\n\n        if (include_tparams__) {\n            size_t alpha_mean_j_2_max__ = d;\n            size_t alpha_mean_j_1_max__ = (2 * n);\n            for (size_t j_2__ = 0; j_2__ < alpha_mean_j_2_max__; ++j_2__) {\n                for (size_t j_1__ = 0; j_1__ < alpha_mean_j_1_max__; ++j_1__) {\n                    param_name_stream__.str(std::string());\n                    param_name_stream__ << \"alpha_mean\" << '.' << j_1__ + 1 << '.' << j_2__ + 1;\n                    param_names__.push_back(param_name_stream__.str());\n                }\n            }\n            size_t alpha_j_2_max__ = d;\n            size_t alpha_j_1_max__ = (2 * n);\n            for (size_t j_2__ = 0; j_2__ < alpha_j_2_max__; ++j_2__) {\n                for (size_t j_1__ = 0; j_1__ < alpha_j_1_max__; ++j_1__) {\n                    param_name_stream__.str(std::string());\n                    param_name_stream__ << \"alpha\" << '.' << j_1__ + 1 << '.' << j_2__ + 1;\n                    param_names__.push_back(param_name_stream__.str());\n                }\n            }\n            size_t overdispersion_scalars_j_1_max__ = (2 * n);\n            for (size_t j_1__ = 0; j_1__ < overdispersion_scalars_j_1_max__; ++j_1__) {\n                param_name_stream__.str(std::string());\n                param_name_stream__ << \"overdispersion_scalars\" << '.' << j_1__ + 1;\n                param_names__.push_back(param_name_stream__.str());\n            }\n        }\n\n        if (!include_gqs__) return;\n    }\n\n\n    void unconstrained_param_names(std::vector<std::string>& param_names__,\n                                   bool include_tparams__ = true,\n                                   bool include_gqs__ = true) const {\n        std::stringstream param_name_stream__;\n        size_t beta_j_2_max__ = (d - 1);\n        size_t beta_j_1_max__ = p;\n        for (size_t j_2__ = 0; j_2__ < beta_j_2_max__; ++j_2__) {\n            for (size_t j_1__ = 0; j_1__ < beta_j_1_max__; ++j_1__) {\n                param_name_stream__.str(std::string());\n                param_name_stream__ << \"beta\" << '.' << j_1__ + 1 << '.' << j_2__ + 1;\n                param_names__.push_back(param_name_stream__.str());\n            }\n        }\n        size_t u_j_1_max__ = n;\n        for (size_t j_1__ = 0; j_1__ < u_j_1_max__; ++j_1__) {\n            param_name_stream__.str(std::string());\n            param_name_stream__ << \"u\" << '.' << j_1__ + 1;\n            param_names__.push_back(param_name_stream__.str());\n        }\n        param_name_stream__.str(std::string());\n        param_name_stream__ << \"sigma_u\";\n        param_names__.push_back(param_name_stream__.str());\n        size_t overdispersion_scalar_k_0_max__ = 2;\n        for (size_t k_0__ = 0; k_0__ < overdispersion_scalar_k_0_max__; ++k_0__) {\n            param_name_stream__.str(std::string());\n            param_name_stream__ << \"overdispersion_scalar\" << '.' << k_0__ + 1;\n            param_names__.push_back(param_name_stream__.str());\n        }\n\n        if (!include_gqs__ && !include_tparams__) return;\n\n        if (include_tparams__) {\n            size_t alpha_mean_j_2_max__ = d;\n            size_t alpha_mean_j_1_max__ = (2 * n);\n            for (size_t j_2__ = 0; j_2__ < alpha_mean_j_2_max__; ++j_2__) {\n                for (size_t j_1__ = 0; j_1__ < alpha_mean_j_1_max__; ++j_1__) {\n                    param_name_stream__.str(std::string());\n                    param_name_stream__ << \"alpha_mean\" << '.' << j_1__ + 1 << '.' << j_2__ + 1;\n                    param_names__.push_back(param_name_stream__.str());\n                }\n            }\n            size_t alpha_j_2_max__ = d;\n            size_t alpha_j_1_max__ = (2 * n);\n            for (size_t j_2__ = 0; j_2__ < alpha_j_2_max__; ++j_2__) {\n                for (size_t j_1__ = 0; j_1__ < alpha_j_1_max__; ++j_1__) {\n                    param_name_stream__.str(std::string());\n                    param_name_stream__ << \"alpha\" << '.' << j_1__ + 1 << '.' << j_2__ + 1;\n                    param_names__.push_back(param_name_stream__.str());\n                }\n            }\n            size_t overdispersion_scalars_j_1_max__ = (2 * n);\n            for (size_t j_1__ = 0; j_1__ < overdispersion_scalars_j_1_max__; ++j_1__) {\n                param_name_stream__.str(std::string());\n                param_name_stream__ << \"overdispersion_scalars\" << '.' << j_1__ + 1;\n                param_names__.push_back(param_name_stream__.str());\n            }\n        }\n\n        if (!include_gqs__) return;\n    }\n\n}; // model\n\n}  // namespace\n\ntypedef model162b0333519e1_stan_DM_ME_namespace::model162b0333519e1_stan_DM_ME stan_model;\n\n#ifndef USING_R\n\nstan::model::model_base& new_model(\n        stan::io::var_context& data_context,\n        unsigned int seed,\n        std::ostream* msg_stream) {\n  stan_model* m = new stan_model(data_context, seed, msg_stream);\n  return *m;\n}\n\n#endif\n\n"

$model_name
[1] "stan_DM_ME"

$model_code
[1] "// Stan model for the dirichlet-multinomial\nfunctions {\n  real dirichlet_multinomial_lpmf(int[] y, vector alpha) {\n    real alpha_plus = sum(alpha);\n    return lgamma(alpha_plus) + sum(lgamma(alpha + to_vector(y)))\n    - lgamma(alpha_plus+sum(y)) - sum(lgamma(alpha));\n  }\n}\n\ndata {\n  int<lower=0> n; // number of samples. \\in N^1\n  int<lower=0> d; // number of signatures. \\in N^1\n  int w[2*n,d]; // number of mutations attributed to each signature. \\in N^{Ns,Nk}\n  int p; // number of covariates\n  matrix[p, 2*n] x; // covariate matrix\n  matrix[n, 2*n] Z; // matrix for random effects\n}\n\nparameters {\n  matrix[p,d-1] beta; // coefficients for fixed effects\n  vector[n] u; // coefficients for random effects\n  real<lower=0> sigma_u; // sd for random effect coefficients\n  real<lower=0> overdispersion_scalar[2]; // for the dirichlet\n}\n\ntransformed parameters {\n  matrix[2*n,d] alpha_mean;\n  matrix<lower=0>[2*n,d] alpha; \n  vector<lower=0>[2*n] overdispersion_scalars = append_row(rep_vector(overdispersion_scalar[1], n),rep_vector(overdispersion_scalar[2], n));\n  \n  alpha_mean = append_col( (x'*beta + Z'*rep_matrix(u, d-1)), rep_vector(0, 2*n));\n  for(l in 1:(2*n)){\n    alpha[l,] = to_row_vector(softmax(to_vector(alpha_mean[l,])))* overdispersion_scalars[l];\n  }\n}\n\nmodel {\n  \n  sigma_u ~ gamma(5, 5);\n\n    overdispersion_scalar ~ normal(1,1);\n  \n\n  for(d_it in 1:(d-1)){\n    beta[,d_it] ~ uniform(-5, 5);\n  }\n  \n// prior for random effects\n  to_vector(u) ~ normal(0, sqrt(sigma_u));\n  \n  for (l in 1:(2*n) ) {\n    w[l,] ~ dirichlet_multinomial(to_vector(alpha[l,]));\n  }\n}\n\n"
attr(,"model_name2")
[1] "stan_DM_ME"

[1] 208
code for methods in class “Rcpp_stan_fit4model162b0141abc64_stan_DM_ME” was not checked for suspicious field assignments (recommended package ‘codetools’ not available?)
code for methods in class “Rcpp_stan_fit4model162b0141abc64_stan_DM_ME” was not checked for suspicious field assignments (recommended package ‘codetools’ not available?)

SAMPLING FOR MODEL 'stan_DM_ME' NOW (CHAIN 1).

SAMPLING FOR MODEL 'stan_DM_ME' NOW (CHAIN 2).
Chain 2: 
Chain 2: Gradient evaluation took 0.01 seconds
Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 100 seconds.
Chain 2: Adjust your expectations accordingly!
Chain 2: 
Chain 2: 
Chain 1: 
Chain 1: Gradient evaluation took 0.01 seconds
Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 100 seconds.
Chain 1: Adjust your expectations accordingly!
Chain 1: 
Chain 1: 
Chain 2: Iteration:     1 / 20000 [  0%]  (Warmup)
Chain 1: Iteration:     1 / 20000 [  0%]  (Warmup)
Chain 2: Iteration:  2000 / 20000 [ 10%]  (Warmup)
Chain 1: Iteration:  2000 / 20000 [ 10%]  (Warmup)
Chain 2: Iteration:  4000 / 20000 [ 20%]  (Warmup)
Chain 1: Iteration:  4000 / 20000 [ 20%]  (Warmup)
Chain 2: Iteration:  6000 / 20000 [ 30%]  (Warmup)
Chain 1: Iteration:  6000 / 20000 [ 30%]  (Warmup)
Chain 1: Iteration:  8000 / 20000 [ 40%]  (Warmup)
Chain 2: Iteration:  8000 / 20000 [ 40%]  (Warmup)
Chain 2: Iteration: 10000 / 20000 [ 50%]  (Warmup)
Chain 2: Iteration: 10001 / 20000 [ 50%]  (Sampling)
Chain 1: Iteration: 10000 / 20000 [ 50%]  (Warmup)
Chain 1: Iteration: 10001 / 20000 [ 50%]  (Sampling)
Chain 2: Iteration: 12000 / 20000 [ 60%]  (Sampling)
Chain 1: Iteration: 12000 / 20000 [ 60%]  (Sampling)
Chain 2: Iteration: 14000 / 20000 [ 70%]  (Sampling)
Chain 1: Iteration: 14000 / 20000 [ 70%]  (Sampling)
Chain 2: Iteration: 16000 / 20000 [ 80%]  (Sampling)
Chain 2: Iteration: 18000 / 20000 [ 90%]  (Sampling)
Chain 1: Iteration: 16000 / 20000 [ 80%]  (Sampling)
Chain 2: Iteration: 20000 / 20000 [100%]  (Sampling)
Chain 2: 
Chain 2:  Elapsed Time: 1710.96 seconds (Warm-up)
Chain 2:                1444.93 seconds (Sampling)
Chain 2:                3155.89 seconds (Total)
Chain 2: 

SAMPLING FOR MODEL 'stan_DM_ME' NOW (CHAIN 3).
Chain 3: 
Chain 3: Gradient evaluation took 0.01 seconds
Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 100 seconds.
Chain 3: Adjust your expectations accordingly!
Chain 3: 
Chain 3: 
Chain 3: Iteration:     1 / 20000 [  0%]  (Warmup)
Chain 1: Iteration: 18000 / 20000 [ 90%]  (Sampling)
Chain 3: Iteration:  2000 / 20000 [ 10%]  (Warmup)
Chain 1: Iteration: 20000 / 20000 [100%]  (Sampling)
Chain 1: 
Chain 1:  Elapsed Time: 1718.09 seconds (Warm-up)
Chain 1:                1995.89 seconds (Sampling)
Chain 1:                3713.98 seconds (Total)
Chain 1: 

SAMPLING FOR MODEL 'stan_DM_ME' NOW (CHAIN 4).
Chain 4: 
Chain 4: Gradient evaluation took 0.01 seconds
Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 100 seconds.
Chain 4: Adjust your expectations accordingly!
Chain 4: 
Chain 4: 
Chain 4: Iteration:     1 / 20000 [  0%]  (Warmup)
Chain 3: Iteration:  4000 / 20000 [ 20%]  (Warmup)
Chain 3: Iteration:  6000 / 20000 [ 30%]  (Warmup)
Chain 4: Iteration:  2000 / 20000 [ 10%]  (Warmup)
